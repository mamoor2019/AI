{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnistcnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPU+jdmpaU/w9GybxMmBhsm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamoor2019/AI/blob/main/mnistcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzhy19-8Sh53"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuu6R77jmAxs",
        "outputId": "bb8ace76-b100-4ac6-9a0c-4b0127e6ebd7"
      },
      "source": [
        "import tensorflow as tf\n",
        "data=tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images,training_labels),(test_images,test_labels)=data.load_data()\n",
        "\n",
        "training_images=training_images.reshape(60000,28,28,1)\n",
        "training_images=training_images/255.0\n",
        "test_images=test_images.reshape(10000,28,28,1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model=tf.keras.models.Sequential([\n",
        "                                  tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)),\n",
        "                                  tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                  tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "                                  tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                  tf.keras.layers.Flatten(),\n",
        "                                  tf.keras.layers.Dense(128,activation=tf.nn.relu),\n",
        "                                  tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images,training_labels,epochs=50)\n",
        "\n",
        "model.evalute(test_images,test_labels)\n",
        "\n",
        "calssifications=model.predict(test_images)\n",
        "\n",
        "print(calssifications[0])\n",
        "print(test_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.4293 - accuracy: 0.8453\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 85s 45ms/step - loss: 0.2840 - accuracy: 0.8963\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 85s 45ms/step - loss: 0.2401 - accuracy: 0.9101\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 84s 45ms/step - loss: 0.2104 - accuracy: 0.9222\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.1846 - accuracy: 0.9304\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 87s 46ms/step - loss: 0.1600 - accuracy: 0.9409\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.1400 - accuracy: 0.9477\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.1232 - accuracy: 0.9530\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.1047 - accuracy: 0.9604\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0934 - accuracy: 0.9652\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0805 - accuracy: 0.9693\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0705 - accuracy: 0.9738\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0634 - accuracy: 0.9758\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0566 - accuracy: 0.9786\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0494 - accuracy: 0.9814\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0471 - accuracy: 0.9826\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0415 - accuracy: 0.9844\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0405 - accuracy: 0.9852\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0385 - accuracy: 0.9862\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0333 - accuracy: 0.9878\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0324 - accuracy: 0.9883\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0261 - accuracy: 0.9906\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0297 - accuracy: 0.9892\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 85s 46ms/step - loss: 0.0304 - accuracy: 0.9887\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0308 - accuracy: 0.9890\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0216 - accuracy: 0.9926\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0267 - accuracy: 0.9908\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0219 - accuracy: 0.9923\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0238 - accuracy: 0.9913\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0211 - accuracy: 0.9928\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0248 - accuracy: 0.9916\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 87s 46ms/step - loss: 0.0224 - accuracy: 0.9925\n",
            "Epoch 33/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0218 - accuracy: 0.9923\n",
            "Epoch 34/50\n",
            "1875/1875 [==============================] - 87s 46ms/step - loss: 0.0192 - accuracy: 0.9934\n",
            "Epoch 35/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0200 - accuracy: 0.9930\n",
            "Epoch 36/50\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0200 - accuracy: 0.9934\n",
            "Epoch 37/50\n",
            "1875/1875 [==============================] - 85s 46ms/step - loss: 0.0210 - accuracy: 0.9933\n",
            "Epoch 38/50\n",
            "1596/1875 [========================>.....] - ETA: 12s - loss: 0.0171 - accuracy: 0.9939"
          ]
        }
      ]
    }
  ]
}